{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8301d98c",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy==1.19.5\n",
    "scipy==1.6.0\n",
    "pandas==1.1.5\n",
    "scikit-learn==0.24.1\n",
    "xgboost==1.2.0\n",
    "tensorflow==2.4.1\n",
    "jupyter>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e181c93",
   "metadata": {},
   "source": [
    "### 1. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b565f",
   "metadata": {},
   "source": [
    "#### A. Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "general_regex = re.compile(r'[aA-zZ]+&[aA-zZ]|[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "\n",
    "\n",
    "def load_stopwords(path: str) -> Dict[str, bool]:\n",
    "    return {sw: True for sw in Path(path).read_text().split('\\n')}\n",
    "\n",
    "\n",
    "def clean_text(text: str, d_stopwords: Dict[str, bool], l_token_filter: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Clean a text string for NLP analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        A text (string) to normalize.\n",
    "\n",
    "    l_token_filter: list of str\n",
    "        Custom token to filter out.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    cleaned_text : str\n",
    "        A text cleaned, needed for transferring text from human language to machine-readable format for further\n",
    "        processing.\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenize_text_pattern(text.lower(), d_stopwords)\n",
    "\n",
    "    if l_token_filter is not None:\n",
    "        l_token_filter = list(map(lambda x: x.lower(), l_token_filter))\n",
    "        tokens = [t for t in tokens if t not in l_token_filter]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize_text_pattern(text: str, d_stopwords: Dict[str, bool]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text\n",
    "\n",
    "    Remove campaigns date, seek for <token>x<token> pattern and <c>&<c> patterns using re.pattern technique.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        text that should be tokenized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of token (str) built from input text.\n",
    "    \"\"\"\n",
    "    # Get token\n",
    "    other_tokens = [x for x in general_regex.findall(text) if len(x) >= 2]\n",
    "\n",
    "    # Remove stopwords\n",
    "    l_tokens = [w for w in other_tokens if not d_stopwords.get(w, False)]\n",
    "\n",
    "    return l_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceded81",
   "metadata": {},
   "source": [
    "#### B. Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45242df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'name', 'pierre', \"i'm\", 'work', 'sia', 'partner', 'month', 'phone', 'number', 'mail', 'sia', 'com']\n"
     ]
    }
   ],
   "source": [
    "path_stopwords = 'nltk_data/stopwords/english'\n",
    "test_text = \"\"\"\n",
    "    Hello ! my name is Pierre I'm 30  and I work at SIA Partner for + 6 month :) my phone number \n",
    "    is 0612 and my e-mail is pg@sia.com \n",
    "\"\"\"\n",
    "\n",
    "d_stopwords = load_stopwords(path_stopwords)\n",
    "l_tokens = clean_text(test_text, d_stopwords, ['pg'])\n",
    "\n",
    "print(l_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a1f60",
   "metadata": {},
   "source": [
    "### 2. AutoEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc718db",
   "metadata": {},
   "source": [
    "#### A. Util class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20ef29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import metrics   \n",
    "\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim: int, n_kernels: int, kernel_size: int, input_dim: Tuple[int, int]):\n",
    "        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # I/O dim\n",
    "        self.latent_dim, self.input_dim = latent_dim, input_dim\n",
    "        \n",
    "        # Kernel's dim\n",
    "        self.n_kernels, self.kernel_size = n_kernels, kernel_size\n",
    "        \n",
    "        # Create encoder / decoder\n",
    "        self.encoder = self.__create_encoder(self.latent_dim, self.n_kernels, self.kernel_size, self.input_dim)\n",
    "        \n",
    "        # TODO: self.decoder = ...\n",
    "        # TODO: train autoencoder (handle padding, etc ...)\n",
    "        \n",
    "    @staticmethod\n",
    "    def __create_encoder(\n",
    "        latent_dim: int, n_kernels: int, kernel_size: int, input_dim: Tuple[int, int]\n",
    "    ) -> Model:\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the encoding layers:\n",
    "        CONV1D -> RELU -> MAXPOOL -> DENSE -> OUTPUT\n",
    "\n",
    "        inputs dim are (n_batch, n_steps, n_embedding)\n",
    "        \n",
    "        In the case of chain of character encoding n_embedding = 36 and n_steps is large enough so that most \n",
    "        of the sentence won't be truncated\n",
    "\n",
    "        \"\"\"\n",
    "        X_input = layers.Input(input_dim)\n",
    "\n",
    "        # 1D conv with activation\n",
    "        X = layers.Conv1D(\n",
    "            n_kernels, kernel_size, activation='relu', padding=\"same\", input_shape=input_dim\n",
    "        )(X_input)\n",
    "\n",
    "        # Max pool layer\n",
    "        X = layers.MaxPooling1D(pool_size=input_dim[0], padding='valid')(X)\n",
    "\n",
    "        # Flatten\n",
    "        X = layers.Flatten()(X)\n",
    "\n",
    "        # End with a dense FC layer\n",
    "        X = layers.Dense(latent_dim, name='output_layer')(X)\n",
    "\n",
    "        model_encoding = Model(inputs=X_input, outputs=X, name='model_embedding')\n",
    "\n",
    "        return model_encoding\n",
    "    \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60afd9",
   "metadata": {},
   "source": [
    "#### B. Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c22b1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 25), dtype=float32, numpy=\n",
       "array([[-1.2880787 ,  3.08959   ,  8.532077  ,  2.198817  ,  1.4167652 ,\n",
       "         0.35796553, -3.099937  , -2.7360947 , -1.5057857 ,  1.1456753 ,\n",
       "         1.1126702 ,  0.57407707, -0.6077388 , -1.3103471 , -2.8989682 ,\n",
       "        -0.5535148 ,  0.04779186,  1.5541102 ,  3.1268792 ,  1.9724679 ,\n",
       "         3.6592827 ,  5.032964  , -3.4500494 ,  3.4744127 , -0.47673264],\n",
       "       [-1.4928756 ,  3.2384596 ,  8.42879   ,  3.2178411 ,  1.4419081 ,\n",
       "         1.3574014 , -2.7454884 , -2.8523047 , -3.067702  ,  1.7219293 ,\n",
       "         0.7400052 ,  0.36554098, -0.82152396, -0.8702831 , -2.797946  ,\n",
       "        -1.3560873 , -0.2812631 ,  1.5519583 ,  2.8478532 ,  1.8341883 ,\n",
       "         4.020779  ,  4.803356  , -3.4221663 ,  3.9082623 , -0.77048296],\n",
       "       [-0.9479112 ,  3.8784826 ,  8.545241  ,  3.4300733 ,  0.6023769 ,\n",
       "         1.0034474 , -2.3583612 , -3.0714471 , -2.2961705 ,  1.2910479 ,\n",
       "         0.59781843,  0.05937263, -0.78640825, -1.1196426 , -2.2951586 ,\n",
       "        -1.3109645 , -0.2242799 ,  1.4790313 ,  2.5243607 ,  1.59635   ,\n",
       "         3.9310045 ,  4.519032  , -3.713995  ,  2.5596912 , -1.0657616 ],\n",
       "       [-0.92335266,  3.9229193 ,  8.323372  ,  2.698579  ,  0.70480263,\n",
       "         0.9972589 , -3.2088969 , -3.6930287 , -2.7472546 ,  0.9290462 ,\n",
       "         0.40116847,  0.02203919, -1.2329905 , -0.66003186, -2.988506  ,\n",
       "        -0.82468885,  0.75025403,  1.1426414 ,  3.7444606 ,  1.6132677 ,\n",
       "         3.4355588 ,  4.8268504 , -3.1401012 ,  3.3874002 , -0.4267986 ],\n",
       "       [-0.58212656,  2.897258  ,  8.656599  ,  2.8163562 ,  0.9313514 ,\n",
       "         0.8843932 , -3.1711855 , -3.3169463 , -2.6482909 ,  1.3755255 ,\n",
       "         0.26514792, -0.08227382, -0.74842864, -0.7835805 , -2.7403276 ,\n",
       "        -0.8729812 , -0.01041179,  1.4778239 ,  2.9916282 ,  2.0274842 ,\n",
       "         4.025913  ,  4.564256  , -3.4725266 ,  3.2139668 , -1.1103327 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "input_dim, latent_dim = (1000, 36), 25\n",
    "n_kernels, kernel_size = 100, 5\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim, n_kernels, kernel_size, input_dim)\n",
    "\n",
    "# Exemple of dataset => building 5 random matrices of shape (1000, 36) [final shape (5, 1000, 36)] :\n",
    "ax_fake_data = np.stack([np.vstack([np.random.randn(36) for i in range(1000)]) for j in range(5)])\n",
    "\n",
    "autoencoder.encoder(ax_fake_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
